{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets.mnist import load_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Load the data\n",
    "(train_digits, train_labels), (test_digits, test_labels) = load_data()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Initialize dictionary to store 5 samples for each digit (0-9)\n",
    "digit_samples = {digit: [] for digit in range(10)}\n",
    "\n",
    "# Loop through each digit and randomly select 5 samples from the training set\n",
    "for digit in range(10):\n",
    "    # Get all indices of the current digit in train_labels\n",
    "    digit_indices = np.where(train_labels == digit)[0]\n",
    "\n",
    "    # Randomly select 5 indices from these\n",
    "    random_indices = np.random.choice(digit_indices, 5, replace=False)\n",
    "\n",
    "    # Collect the images of the chosen indices\n",
    "    digit_samples[digit] = train_digits[random_indices]\n",
    "\n",
    "# Plot the samples in a 10x5 grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for digit, images in digit_samples.items():\n",
    "    for i, img in enumerate(images):\n",
    "        plt.subplot(10, 5, digit * 5 + i + 1)  # Position in 10x5 grid\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.ylabel(f\"Digit {digit}\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"5 Random Samples of Each Digit (0-9)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the path to your dataset in Google Drive\n",
    "# Replace with your actual path inside your Google Drive\n",
    "data_dir = '/content/drive/MyDrive/MNIST_Dataset'\n",
    "\n",
    "# Step 2: Prepare the Image Data\n",
    "# Using ImageDataGenerator to load images and apply preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,           # Normalize pixel values between 0 and 1\n",
    "    validation_split=0.2         # Use 20% of data for validation\n",
    ")\n",
    "\n",
    "# Load training data from the folders\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(28, 28),        # Resize images to 28x28 pixels\n",
    "    color_mode=\"grayscale\",      # Convert images to grayscale\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",    # Use categorical labels (one-hot encoding)\n",
    "    subset=\"training\"            # Training subset\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(28, 28),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"          # Validation subset\n",
    ")\n",
    "\n",
    "# Step 3: Build the Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Output layer with 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the Model\n",
    "epochs = 10  # Number of epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the Model (optional)\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() # Load test data\n",
    "\n",
    "# ... (rest of your existing code for X_train processing) ...\n",
    "\n",
    "# Some variables...\n",
    "image_height = train_digits.shape[1]\n",
    "image_width = train_digits.shape[2]\n",
    "num_channels = 1  # we have grayscale images\n",
    "\n",
    "# Reshape the images data\n",
    "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
    "test_data = np.reshape(test_digits, (test_digits.shape[0], image_height, image_width, num_channels))\n",
    "\n",
    "# Rescale the image data to values between (0.0,1.0]\n",
    "train_data = train_data.astype('float32') / 255.\n",
    "test_data = test_data.astype('float32') / 255.\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = 10\n",
    "train_labels_cat = to_categorical(train_labels, num_classes)\n",
    "test_labels_cat = to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training dataset (5 times!)\n",
    "for _ in range(5):\n",
    "    indexes = np.random.permutation(len(train_data))\n",
    "\n",
    "train_data = train_data[indexes]\n",
    "train_labels_cat = train_labels_cat[indexes]\n",
    "\n",
    "# now set-aside 10% of the train_data/labels as the\n",
    "# cross-validation sets\n",
    "val_perc = 0.10\n",
    "val_count = int(val_perc * len(train_data))\n",
    "\n",
    "# first pick validation set from train_data/labels\n",
    "val_data = train_data[:val_count,:]\n",
    "val_labels_cat = train_labels_cat[:val_count,:]\n",
    "\n",
    "# leave rest in training set\n",
    "train_data2 = train_data[val_count:,:]\n",
    "train_labels_cat2 = train_labels_cat[val_count:,:]\n",
    "\n",
    "# NOTE: We will train on train_data2/train_labels_cat2 and\n",
    "# cross-validate on val_data/val_labels_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib to create plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_plots(history):\n",
    "    \"\"\"\n",
    "    Displays plots for accuracy and loss during training.\n",
    "\n",
    "    Args:\n",
    "        history: The training history object returned by model.fit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # add Convolutional layers\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
    "                     input_shape=(image_height, image_width, num_channels)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    # Densely connected layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile with adam optimizer & categorical_crossentropy loss function\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive to save the trained model\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_digits, train_labels), (test_digits, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_digits = train_digits.reshape((train_digits.shape[0], 28, 28, 1)).astype(\"float32\") / 255\n",
    "test_digits = test_digits.reshape((test_digits.shape[0], 28, 28, 1)).astype(\"float32\") / 255\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "train_labels_cat = to_categorical(train_labels, 10)\n",
    "test_labels_cat = to_categorical(test_labels, 10)\n",
    "\n",
    "# Build a CNN model\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the model\n",
    "model = build_model()\n",
    "model.fit(train_digits, train_labels_cat, epochs=10, batch_size=64, validation_data=(test_digits, test_labels_cat))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_digits, test_labels_cat)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model to Google Drive\n",
    "model.save('/content/drive/MyDrive/MNIST_Dataset/my_model.h5')  # Replace with your desired model path and name\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(train_data2, train_labels_cat2,\n",
    "                    epochs=15, batch_size=64,\n",
    "                    validation_data=(val_data, val_labels_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plots...\n",
    "show_plots(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = \\\n",
    "  model.evaluate(test_data, test_labels_cat, batch_size=64)\n",
    "print('Test loss: %.4f accuracy: %.4f' % (test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "first20_preds = np.argmax(predictions, axis=1)[:25]\n",
    "first20_true = np.argmax(test_labels_cat,axis=1)[:25]\n",
    "print(first20_preds)\n",
    "print(first20_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many mismatches?\n",
    "num_mismatches = (np.argmax(predictions, axis=1) != np.argmax(test_labels_cat, axis=1)).sum()\n",
    "print(\"Number of mismatches:\", num_mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import drive\n",
    "from tensorflow import keras\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the trained model (change 'your_model_path' to the actual path)\n",
    "model = keras.models.load_model('/content/drive/MyDrive/MNIST_Dataset/my_model.h5', compile=False)\n",
    "\n",
    "# Define a function to preprocess the image and make predictions\n",
    "def preprocess_and_predict(image_path, model, threshold=0.1):\n",
    "    \"\"\"Loads an image, preprocesses it, and predicts using the model.\"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        input_image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "        input_image_resize = input_image.resize((28, 28))  # Resize to 28x28\n",
    "\n",
    "        # Convert image to array and normalize\n",
    "        input_image_array = np.array(input_image_resize) / 255.0\n",
    "        image_reshaped = np.reshape(input_image_array, (1, 28, 28, 1))  # Reshape to (1, 28, 28, 1)\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(image_reshaped)\n",
    "        max_prob = np.max(prediction)\n",
    "        predicted_label = np.argmax(prediction)\n",
    "\n",
    "        # Check threshold\n",
    "        if max_prob >= threshold:\n",
    "            return predicted_label, max_prob\n",
    "        else:\n",
    "            return \"Uncertain\", max_prob\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Folder path in Google Drive containing images (replace with actual folder path)\n",
    "image_folder_path = '/content/drive/MyDrive/MNIST_Images/'\n",
    "\n",
    "# Loop through each image in the folder and predict\n",
    "for image_filename in os.listdir(image_folder_path):\n",
    "    image_path = os.path.join(image_folder_path, image_filename)\n",
    "\n",
    "    if image_path.endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image files\n",
    "        predicted_label, confidence = preprocess_and_predict(image_path, model, threshold=0.9)\n",
    "        print(f\"Image: {image_filename} - Predicted Digit: {predicted_label}, Confidence: {confidence}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
